---
title: "A Preference for Interactions? (Know Your Implicit Priors!)"
layout: post
category: posts
draft: true
---

Todo:

- label the predictions charts, e.g. "here we're making predictions about data unlike that seen in the training set" - maybe also label the bottom line in both charts? "influence of X2 is the same here as when X1=1", "X2 makes no difference to the prediction when X1=0"

Often we think of "interactions" (when the effect of one feature differs depending on the value of another) as interesting/surprising discoveries. Given that, we often use models that prefer not using interactions, introducing them only when the evidence warrants. In this post, I'll use a simple toy example to walk through why decision trees and ensembles of decision trees (Random Forests) do just the opposite: When the evidence is equally consistent with an interaction effect and no interaction, they prefer the interaction effect.

I'll also look at a neat Bayesian machine learning algorithm, [Bayesian Additive Regression Trees](http://cran.r-project.org/web/packages/bartMachine/vignettes/bartMachine.pdf) (BART), and show how the parameters controlling the prior distribution affect the model's tendency to learn an interaction effect.

Suppose you're given this data and asked to make a prediction about `$X_1 = 0$, $X_2 = 1$`.

```{r echo=FALSE, message=FALSE, warning=FALSE}
library(ggplot2)
library(knitr)
library(dplyr)
ColorScale <- function(...) {
  return(scale_color_gradientn(colours = c('#3288bd','#99d594','#e6f598', '#fee08b', '#fc8d59', '#d53e4f'), na.value="lightgrey", ...))
}


nTrain <- 100
X1 <- sample(c(0, 1), size=nTrain, replace=TRUE)
X2 <- ifelse(X1 == 0, 0, sample(c(0, 1), size=nTrain, replace=TRUE))
train <- data.frame(X1, X2, stringsAsFactors=TRUE)
train$Y <-  5 + 10*X1 + 2*X2

uniques <- train %>% group_by(X1, X2) %>% summarise(Y = mean(Y), NumTrain = n())
train$Y <- train$Y + rnorm(n=nTrain, sd=.2)
uniques$Y <- sprintf("Y = %d + small noise", uniques$Y)
uniquesWithQuestionMark <- rbind(uniques, data.frame(X1 = 0, X2 = 1, Y = "?", NumTrain = 0)) %>% rename("N Training Rows:" = NumTrain) %>% 
  data.frame(check.names=FALSE)

uniquesForLabelingPlot <- transform(uniques, X1 = X1 + 1.1, X2 = X2 + 1.1)

ggplot(train) + 
  geom_point(aes(x=factor(X1), y=factor(X2), color=Y), position=position_jitter(.03, .03)) + 
  geom_text(aes(x=factor(0), y=factor(1), label=I("?")), size=30) +
  geom_text(aes(x=X1, y=X2, label=Y), data=uniquesForLabelingPlot) +
  ColorScale() + 
  ggtitle("The Region of Feature Space Marked '?' Has No Training Data") + 
  xlab("X1") + ylab("X2")

```

```{r results='asis', echo=FALSE}
library(xtable)
print(xtable(uniquesWithQuestionMark, digits=rep(0,5), align=rep("c", 5)), type="html", include.rownames=FALSE)
```

In practice making an inference at `$X_1 = 0$, $X_2 = 1$` is pretty hopeless. The training data doesn't help much, so your prediction will depend entirely on your priors. But that's exactly why I'm using this example to get at what the priors are in various models. Real problems will have pieces in common with this example, so it helps get a handle on how models will behave for those problems.

(In fact, if for some reason you had this example in real life, you probably wouldn't even bother with a model. It reminds me of an old criticism of Bayesian reasoning: *If you can really know your prior (what you should think given all previous information), why bother with a model rather than just look at the data and report what you think afterward (your new 'prior') as the answer?* In this case, that's probably exactly what you *should* do.)


### A Linear Model

```{r}
lmFit <- lm(Y ~ X1 + X2, data = train)
```

If you fit a linear model of the form `$\mathbb{E}[Y] = \beta_0 + \beta_1 X_1 + \beta_2 X_2$`, you find

`$$\mathbb{E}[Y] = 5 + 10 X_1 + 2 X_2.$$`

This fits the training data perfectly and extrapolates to the unseen region of feature space using the assumption that effects are additive.

```{r results='asis', echo=FALSE}
predictionFeatures <- expand.grid(X1 = 0:1,  X2 = 0:1)
predictions <- transform(predictionFeatures, LmPrediction = predict(lmFit, predictionFeatures))

ggplot(predictions) + geom_line(aes(x=factor(X2), y=LmPrediction, color=factor(X1), group=X1)) +
  ggtitle("Linear Model Predictions: Effect of X2 Does Not Depend on X1") + 
  xlab("X2") +
  scale_color_discrete(name = "X1")

```

The line for `$X_1 = 0$` is parallel to the one for `$X_1 = 1$`, meaning that the influence of `$X_2$` is the same ($+2$) regardless of the value of `$X_1$`


### Random Forest (and decision trees)

```{r echo=FALSE, message=FALSE}
library(randomForest)
```

Fitting a random forest and plotting its predictions, we see that it makes the same predictions as the linear model where we have training data (which fit the data perfectly), but has decided that `$X_2$` only matters when `$X_1 = 1$`:

```{r}
rfFit <- randomForest(Y ~ X1 + X2, data = train, mtry=2)
```

```{r echo=FALSE}
predictions$RfPrediction <- predict(rfFit, predictions)
ggplot(predictions) + geom_line(aes(x=factor(X2), y=RfPrediction, color=factor(X1), group=X1)) +
  ggtitle("Random Forest Predictions: X2 only matters when X1=1") + 
  xlab("X2") +
  scale_color_discrete(name = "X1")
```

It's easy to understand from the trees why this happened. In this simple example, all of the trees are the same, so it's just as if we had one decision tree:

![tree](images/posts/tree.png)


You'll notice I set `mtry=2`. This tells the random forest to consider both variables at each split (normally you consider only a randomly chosen subset of the variables for each split). This choice makes the example clearer. I don't consider that "cheating" because with the default settings, the random forest fails to replicate even the training data (where there should be no question what predictions are correct).

## Linear Regression with Regularized Interaction Term

We can add an interaction term (with a prior pushing it toward zero). The model is:

`$$Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_{23} X_2 X_3 + N(0,\sigma).$$`

In this version all parameters except `$\beta_{23}$` have improper flat priors while `$$\beta_{23} \sim N(0,3).$$`

Such a simple model doesn't need a flexible tool like Stan, but I like how clear and explicit a model is when you write it down in the Stan language:

```{r, message=FALSE, results='hide', warning=FALSE}
library(rstan)

stanModel1 <- "
data {
  int<lower=0> N;
  vector[N] X1;
  vector[N] X2;
  vector[N] Y;
}
parameters {
  real beta0;
  real beta1;
  real beta2;
  real beta12;
  real<lower=0> sigma;
}
model {
  beta12 ~ normal(0, 2);
  Y ~ normal(beta0 + beta1*X1 + beta2*X2 + beta12*X1 .* X2, sigma);
}
"

stanData <- c(as.list(train), N=nrow(train))
stanFit1 <- stan(model_code=stanModel1, data=stanData, chains=1)

samples1 <- extract(stanFit1) %>% data.frame
samples1$SampleNumber <- 1:nrow(samples1)

stan1Predictions <- merge(samples1, predictionFeatures) %>% 
  mutate(SamplePredictedY = beta0 + beta1*X1 + beta2*X2 + beta12*X1*X2)
```

Now instead of one prediction for each point in feature space, we have a set of posterior samples:

```{r warning=FALSE}
ggplot(stan1Predictions) + 
  geom_line(aes(x=factor(X2), y=SamplePredictedY, color=factor(X1), group=paste(X1, SampleNumber)), alpha=.2) +
  ggtitle("Posterior Samples") + 
  xlab("X2") +
  scale_color_discrete(name = "X1")
```

There's no variation in the predictions for points in feature space that we saw in the training data, but the predicted effect of `$X_2$` varies a lot when `$X_1 = 0$`.

The posterior for the interaction term is actually the same as the prior, which makes sense because the data don't tell you anything about whether there's an interaction:

```{r echo=FALSE}
qplot(beta12, data=samples1) + ggtitle("Posterior samples for interaction parameter")
```

Looking at histograms of posterior samples for predictions, we again see basically no variation at the points where we have training data:

```{r echo=FALSE}
ggplot(stan1Predictions %>% transform(X1 = factor(X1, levels = c("0","1")), X2 = factor(X2, levels=c("1","0")))) + 
  geom_histogram(aes(x=SamplePredictedY)) + 
  facet_grid(X2 ~ X1, scales="free_y", labeller=function(variable, value) sprintf("%s = %s", variable, value)) + 
  ggitle("Posterior Samples for Predictions")
```

Looking closer at the posterior samples at `$X_1 = 0$, $X_2 = 1$`, we see that the predictions are centered on $7$ (the prediction from our model with no interaction), but has substantial variation in both directions:

```{r echo=FALSE}
ggplot(subset(stan1Predictions, X1==0 & X2==1)) + geom_histogram(aes(x=SamplePredictedY))

```

When the interaction term `$\beta_{12}$` is high, `$\beta_2$` makes up for it by being low (and vice versa):
```{r}
qplot(beta2, beta12, data=samples1)
````



## Linear Regression with Interaction Term and Main Effects Regularized

I won't walk through this situation in detail, but note if we put a prior centered on $0$ on `$\beta_2$` as well as `$\beta_{12}$`, the predictions at `$X_1 = 0$, $X_2 = 1$` would shift to the left. In that case, parameters choices with a negative interaction term would be penalized twice: once for the negative `$\beta_{12}$`, and again for pushing `$\beta_2$` higher than it otherwise had to be.

## BART


```{r}
library(reshape2)
library(bartMachine)
nIterAfterBurnIn <- 50000
bartFit <- bartMachine(train[c("X1","X2")], train$Y, 
                       num_burn_in=50000, 
                       num_trees=10, 
                       num_iterations_after_burn_in=nIterAfterBurnIn)
predict(bartFit, predictionFeatures)

bartPredictions <- bartMachine::bart_machine_get_posterior(bartFit, predictionFeatures) %>%
  data.frame %>%
  cbind(predictionFeatures) %>%
  melt(id=c("X1", "X2"), value.name="BartSample") %>%
  transform(X1 = factor(X1, levels = 0:1), X2 = factor(X2, levels=1:0))

ggplot(bartPredictions) + geom_histogram(aes(x=BartSample)) + facet_grid(X2 ~ X1)



# bartMachine:::get_tree_depths(bartFit)
destroy_bart_machine(bartFit)


```

```{r}
# library(reshape2)
# nIterAfterBurnIn <- 10000
# bartFit <- bartMachine(train[c("X1","X2")], train$Y, 
#                        num_burn_in=5000, 
#                        num_trees=20, 
#                        #                        alpha = .5,
# #                        beta = 5,
#                        num_iterations_after_burn_in=nIterAfterBurnIn)
# predict(bartFit, predictionFeatures)
# 
# bartPredictions <- 
#   bartMachine::bart_machine_get_posterior(bartFit, predictionFeatures)$y_hat_posterior_samples %>%
#   data.frame %>%
#   setNames(paste0("Sample", 1:(nIterAfterBurnIn))) %>%
#   cbind(predictionFeatures) %>%
#   melt(id=c("X1", "X2"), value.name="BartSample") %>%
#   transform(X1 = factor(X1, levels = 0:1), X2 = factor(X2, levels=1:0))
# 
# ggplot(bartPredictions) + geom_histogram(aes(x=BartSample)) + facet_grid(X2 ~ X1)
# 
# 
# depths <- bartMachine:::get_tree_depths(bartFit)
# onlyAdditive <- apply(depths, 1, max)==1
# exactlyOneTreeDepth2 <- apply(depths, 1, function(depths) sum(depths==2)==1)==1
# atLeast2TreesDepth2 <- apply(depths, 1, function(depths) sum(depths==2)>1)==1
# 
# preds <- bartMachine::bart_machine_get_posterior(bartFit, predictionFeatures)
# 
# PlotForSubset <- function(subsetIndices) {
#   bartPredictions <- 
#     preds$y_hat_posterior_samples[,subsetIndices] %>%
#     data.frame %>%
#     setNames(paste0("Sample", 1:(sum(subsetIndices)))) %>%
#     cbind(predictionFeatures) %>%
#     melt(id=c("X1", "X2"), value.name="BartSample") %>%
#     transform(X1 = factor(X1, levels = 0:1), X2 = factor(X2, levels=1:0))
#   
#   p <- ggplot(bartPredictions) + geom_histogram(aes(x=BartSample)) + facet_grid(X2 ~ X1)
#   print(p)  
# }
# 
# PlotForSubset(onlyAdditive)
# PlotForSubset(exactlyOneTreeDepth2)
# PlotForSubset(atLeast2TreesDepth2)
# 
# destroy_bart_machine(bartFit)
# 

```

