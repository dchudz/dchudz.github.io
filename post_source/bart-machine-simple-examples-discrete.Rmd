---
title: "Bayesian Trees: Looking for Sensible Results in Simple Examples [source]()"
layout: post
category: posts
draft: true
---

```{r echo=FALSE, message=FALSE, results='hide'}
library(ggplot2)
library(plyr)
library(bartMachine)
```

The Bayesian Additive Regression Trees (BART) model consists of a sum of trees, along with values at the leaf notes. I'm attracted to the method as a way to obtain similar out-of-the-box (no hand tuning or model selection needed) performance to common machine learning methods (random forests, gradient boosting machines, etc.), but also provide **uncertainties** along with each individual prediction. BART is an MCMC method, meaning that we receive a series of draws from the posterior distribution, each of which is a set of tree structures along with values at the leaf nodes.

The purpose of this post is to explore how appropriate the resulting uncertainties seem in a few simple simulated examples where we know the truth. For this, I use the [`bartMachine`]() implementation by SOMEONE and SOMEONEELSE. The above description of the algorithm might be enough to follow along with this post, but I highly recommend reading at least the first page of the package authors' [vignette](), which provides a very clear description of the algorithm.

### Example 1: Simplest possible example

In this example, $X_1$ (which is $0$ for 10 observations and $1$ for 100) is the only input, and 

`$$y = X_1 + 5 + \mathcal{N}(0,1)$$`

The model assumes that the noise is independent of everything, which is consistent with the example. You can see that the mean of $y$ depends on $X_1$ and that we have much more data with `$X_1=1$` than `$X_1=0$`. 

First I'll generate and display the simulated training data:

```{r echo=FALSE}
set.seed(0)
sigmaTrue <- 1
train <- data.frame(X1 = sample(c(rep(0,10), rep(1, 100))))
test <- unique(train)
y <- train$X1 + 5 + rnorm(nrow(train))*sigmaTrue
qplot(train$X1, y, alpha=I(.5)) + ggtitle("Example 1 Training Data")
```

We fit the BART model and plot `$\mathcal{E}(y|X)$   and see that as we'd expect uncertainty is higher for the 0's (where we have fewer observations)

```{r echo=FALSE, message=FALSE, results='hide'}
bartFit <- bartMachine(X = train, y = y)
posteriorSamples <- bart_machine_get_posterior(bartFit, test)$y_hat_posterior_samples

sampleDF <- ldply(1:ncol(posteriorSamples), function(ixSample) {
  testOneSample <- test
  testOneSample$SampleY <- posteriorSamples[,ixSample]
  return(testOneSample)
})      
```

```{r}
ggplot(sampleDF) + geom_point(aes(x=factor(X1), y=SampleY), alpha=.5)
ggplot(sampleDF) + geom_histogram(aes(fill=factor(X1), x=SampleY))
```

<!--more-->


# A little bit more noise shows the difference between one tree and two trees more clearly

```{r}
set.seed(0)
sigmaTrue <- 3
nTrain <- 300
coef <- 2
test <- merge(data.frame(X1=c(0,1)), data.frame(X2=c(0,1)))
train <- test[sample(c(rep(1,nTrain*.45), rep(2,nTrain*.05), rep(3, nTrain*.05), rep(4, nTrain*.45))), ]
ggplot(train) + geom_bar(aes(x=factor(X1), fill=factor(X2)))
y <- coef*train$X1 + coef*train$X2 + rnorm(nrow(train))*sigmaTrue
numTreesList <- c(1,2,100)
names(numTreesList) <- numTreesList
# Default bartMachine
bartFitByNumTrees <- Map(function(numTrees) bartMachine(X = train, y = y, num_trees=numTrees), numTreesList)
posteriorSamplesByNumTrees <- Map(function(bartFit) bart_machine_get_posterior(bartFit, test)$y_hat_posterior_samples, bartFitByNumTrees)

sampleDFByNumTrees <- Map(function(posteriorSamples) {
  return(ldply(1:ncol(posteriorSamples), function(ixSample) {
    testOneSample <- test
    testOneSample$SampleY <- posteriorSamples[,ixSample]
    return(testOneSample)}))},
  posteriorSamplesByNumTrees)

sampleDF <- ldply(names(sampleDFByNumTrees), function(numTreesString) {
  oneSampleDF <- sampleDFByNumTrees[[numTreesString]]
  oneSampleDF$NumTrees <- as.integer(numTreesString)
  return(oneSampleDF)
})
```

```{r}
ggplot(sampleDF) + geom_histogram(aes(fill=factor(X1), x=SampleY), alpha=.6, position=position_identity()) + 
#   geom_vline(aes(xintercept = SampleY, color=factor(X1)), data=ddply(sampleDF, c("X1", "X2", "NumTrees"),
#                                                                      summarize,
#                                                                      SampleY = median(SampleY))) +
  facet_grid(X2 + NumTrees ~ ., labeller=function(variable, value) sprintf("%s: %s", variable, value))
```


# With only 1 tree, the posterior for the X1!=X2 cases is very diffuse and even multi-modal
# This is the right answer when your prior doesn't have an additive structure:
# If X1=1, X2=0... will it behave like X1? Like X2? Or like the X1=1, X2=0 cases you saw in the training data? All of these are possible.

# Most of the trees in the single-tree model have depth 2, which is as you'd expect: 
# That's the right depth to capture the relevant variation
```{r}
qplot(factor(as.vector(bartMachine:::get_tree_depths(bartFitByNumTrees[["1"]])))) +
  ggtitle("Distribution of Tree Depths for Single-Tree Model")
```

# 1/1 ("correct" model) is most common distribution of tree depths, 
```{r}
qplot(apply(bartMachine:::get_tree_depths(bartFitByNumTrees[["2"]]), 1, function(numbers) do.call(paste, as.list(sort(numbers))))) + 
  ggtitle("Distribution of Tree Depths for Two-Tree Model")
```

# Harder to fully understand the distribution of tree depths in the 100-tree model, but here's one example:
```{r}
bartMachine:::get_tree_depths(bartFitByNumTrees[["100"]])[1,]
```

# You might ask why more than 2 of the trees need to have any splits at all. 
# The answer is that the prior for the leafs scales with the number of trees, so that with 100 trees it's very concentrated on 0.
# So it's only the sum of a larger number of trees that can get us away from 0 at all
# For each iteration, we can count how many trees we have at each possible depth. Then look at the distribution of those numbers:

```{r}
treeDepthDF <- ldply(0:2, function(depth) {
  return(data.frame(Depth=depth,
                    NumTreesAtDepth=apply(bartMachine:::get_tree_depths(bartFitByNumTrees[["100"]]), 1, 
                                          function(numbers) sum(numbers==depth))))
})

ggplot(treeDepthDF) + 
  geom_histogram(aes(x=NumTreesAtDepth, fill=factor(Depth)), position=position_identity()) +
  ggtitle("Distribution Across Iteration of Number of Trees at Each Depth")
```

# We see that for all of the iterations, at least half the trees are of depth one which is consistent with the (correct) linear model. 
# A decent number have depth 2, though. Note that for the X1 != X2 observations, these depth-2 trees will have very little influence on the final result: The leafs' prior is strongly concentrated on 0, and the likelihood function will be very diffuse.